{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# url = 'http://eurang.net?'\n",
    "# webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = 'C:/dev_python/Webdriver/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://cafe.naver.com/firenze.cafe\")\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# import time\n",
    "# from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# driver = webdriver.chrome(\"C:/dev_python/Webdriver/chromedriver.exe\")\n",
    "# driver.get(\"http://eurang.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# driver.get(\"http://eurang.net\")\n",
    "driver.find_element_by_xpath('//*[@id=\"cafe-menu\"]/div[2]/div[10]/h3/a').click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"menuLink75\"]').click()\n",
    "\n",
    "# time.sleep(3)\n",
    "\n",
    "# driver.find_element_by_xpath('//*[@id=\"query\"]').click()\n",
    "\n",
    "# time.sleep(1)\n",
    "# driver.find_element_by_xpath('//*[@id=\"menuLink23\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import webbrowser\n",
    "\n",
    "# naver_search_url = \"https://cafe.naver.com/firenze.cafe/search.naver?query=\"\n",
    "# search_word = '이탈리아'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "\n",
    "req = driver.page_source\n",
    "soup=BeautifulSoup(req, 'html.parser')\n",
    "information_list = soup.select(\"#main-area > div:nth-child(9) > form > table > tbody > tr:nth-child(1) > td.board-list > span > span\")\n",
    "information_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SelectorSyntaxError",
     "evalue": "Invalid character '/' position 0\n  line 1:\n//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[3]/td[2]/span/span/a[1]\n^ (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3326\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-35-47eaa2e06101>\"\u001b[0m, line \u001b[0;32m7\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    titles = soup_music.select('//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[3]/td[2]/span/span/a[1]')\n",
      "  File \u001b[0;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\bs4\\element.py\"\u001b[0m, line \u001b[0;32m1334\u001b[0m, in \u001b[0;35mselect\u001b[0m\n    return soupsieve.select(selector, self, namespaces, limit, **kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\soupsieve\\__init__.py\"\u001b[0m, line \u001b[0;32m114\u001b[0m, in \u001b[0;35mselect\u001b[0m\n    return compile(select, namespaces, flags, **kwargs).select(tag, limit)\n",
      "  File \u001b[0;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\soupsieve\\__init__.py\"\u001b[0m, line \u001b[0;32m63\u001b[0m, in \u001b[0;35mcompile\u001b[0m\n    return cp._cached_css_compile(pattern, namespaces, custom, flags)\n",
      "  File \u001b[0;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py\"\u001b[0m, line \u001b[0;32m205\u001b[0m, in \u001b[0;35m_cached_css_compile\u001b[0m\n    CSSParser(pattern, custom=custom_selectors, flags=flags).process_selectors(),\n",
      "  File \u001b[0;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py\"\u001b[0m, line \u001b[0;32m1064\u001b[0m, in \u001b[0;35mprocess_selectors\u001b[0m\n    return self.parse_selectors(self.selector_iter(self.pattern), index, flags)\n",
      "  File \u001b[0;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py\"\u001b[0m, line \u001b[0;32m915\u001b[0m, in \u001b[0;35mparse_selectors\u001b[0m\n    key, m = next(iselector)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\raina\\Anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py\"\u001b[1;36m, line \u001b[1;32m1057\u001b[1;36m, in \u001b[1;35mselector_iter\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise SelectorSyntaxError(msg, self.pattern, index)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32munknown\u001b[0m\n\u001b[1;31mSelectorSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Invalid character '/' position 0\n  line 1:\n//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[3]/td[2]/span/span/a[1]\n^\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[3]/td[2]/span/span/a[1]')\n",
    "titles[0:7]\n",
    "music_titles = [title.get_text() for title in titles]\n",
    "music_titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고코드\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import winsound\n",
    "import requests\n",
    "import configparser\n",
    "import urllib.parse\n",
    "import logging.handlers\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from openpyxl import *\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import UnexpectedAlertPresentException\n",
    "\n",
    "from pattern_search import pat_transform, pat_find, pat_check\n",
    "\n",
    "\n",
    "# 전역변수\n",
    "root_dir = ''\n",
    "setting_dir = ''\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'}\n",
    "driver_loc = '/chromedriver.exe'\n",
    "input_data = list()\n",
    "blacklist = list()\n",
    "conf = {\n",
    "    'max_try': 50,   # 로그인 최대 시도 횟수. (0이면 바로 수동로그인)\n",
    "    'alert': 3,     # 비프음 횟수. (0이면 비프 알림 없음)\n",
    "    'max_page': 0   # 각 url당 수집할 최대 페이지. (0이면 모두 수집)\n",
    "}\n",
    "ban_list = {\n",
    "    'main': list(), # 본문 수집 제외 문구 리스트\n",
    "    'comment': list()   # 댓글 수집 제외 문구 리스트\n",
    "}\n",
    "\n",
    "\n",
    "def alert(a, b):\n",
    "    for i in range(1, conf['alert'] + 1):\n",
    "        winsound.Beep(a, 500)\n",
    "        winsound.Beep(b, 500)\n",
    "        winsound.Beep(a, 500)\n",
    "        winsound.Beep(b, 500)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "def load_setting():\n",
    "    path_dir = root_dir + '/setting'\n",
    "    file_list = os.listdir(path_dir)\n",
    "    file_list.sort()\n",
    "\n",
    "    # 수집 제외 문구 리스트 ( 본문 )\n",
    "    with open(setting_dir + '/ban_list_main.txt', 'r') as bf:\n",
    "        for line in bf.readlines():\n",
    "            ban_list['main'].append(pat_transform(line.strip()))\n",
    "\n",
    "    # 수집 제외 문구 리스트 ( 댓글 )\n",
    "    with open(setting_dir + '/ban_list_comment.txt', 'r') as bf:\n",
    "        for line in bf.readlines():\n",
    "            ban_list['comment'].append(pat_transform(line.strip()))\n",
    "\n",
    "    # 블랙 리스트\n",
    "    with open(setting_dir + '/blacklist.txt', 'r') as bf:\n",
    "        for line in bf.readlines():\n",
    "            blacklist.append(line.strip().replace('@naver.com', ''))\n",
    "    \n",
    "    # 프로그램 세팅 파일\n",
    "    with open(setting_dir + '/program_setting.txt', 'r') as bf:\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(setting_dir + '/program_setting.txt')\n",
    "        conf['max_try'] = int(config['LOGIN']['MAX_TRY'])\n",
    "        conf['alert'] = int(config['LOGIN']['BEEP_ALERT'])\n",
    "        conf['max_page'] = int(config['PROGRAM']['MAX_PAGE'])\n",
    "        logger.info('[SETTING] 로그인 최대 시도 횟수 : {}'.format(conf['max_try']))\n",
    "        logger.info('[SETTING] 자동 로그인 실패 시 알림 횟수 : {}'.format(conf['alert']))\n",
    "        logger.info('[SETTING] 각 카페 당 최대 수집 페이지 수 : {}'.format(conf['max_page']))\n",
    "\n",
    "\n",
    "    for iter in file_list:\n",
    "        file_name = '{0}/{1}'.format(path_dir, iter)\n",
    "\n",
    "        with open(file_name, 'r') as f:\n",
    "            temp_dict = {'keywords': list()}\n",
    "\n",
    "            lines = f.readlines()\n",
    "            now = 'url'\n",
    "            for line in lines:\n",
    "                if line[0] == '#':\n",
    "                    if 'url' in line.strip():\n",
    "                        now = 'url'\n",
    "                    elif 'id / pw' in line.strip():\n",
    "                        now = 'account'\n",
    "                    elif 'keyword' in line.strip():\n",
    "                        now = 'keyword'\n",
    "                    elif 'excel' in line.strip():\n",
    "                        now = 'excel'\n",
    "                    elif '전체' in line.strip():\n",
    "                        now = 'all_crawl'\n",
    "                else:\n",
    "                    if line.strip() == '':\n",
    "                        continue\n",
    "\n",
    "                    if now == 'url':\n",
    "                        temp_dict['url'] = line.strip()\n",
    "                    elif now == 'account':\n",
    "                        temp_dict['id'] = line.split(' ')[0].strip()\n",
    "                        temp_dict['pw'] = line.split(' ')[1].strip()\n",
    "                    elif now == 'keyword':\n",
    "                        temp_dict['keywords'].append(line.strip())\n",
    "                    elif now == 'excel':\n",
    "                        temp_dict['excel_name'] = line.strip()\n",
    "                    elif now == 'all_crawl':\n",
    "                        string = line.strip()\n",
    "                        if string in ['True', 'true']:\n",
    "                            temp_dict['all_crawl'] = True\n",
    "                        else:\n",
    "                            temp_dict['all_crawl'] = False\n",
    "            input_data.append(temp_dict)\n",
    "\n",
    "\n",
    "def _get_now_time():\n",
    "    now = time.localtime()\n",
    "    s = \"{0}.{1:0>2}.{2:0>2}. {3:0>2}:{4:0>2}:{5:0>2}\".format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "    return s\n",
    "\n",
    "\n",
    "def set_excel(file_name):\n",
    "    now_time = time.strftime('%Y%m%d%H%M%S', time.localtime())\n",
    "    FILENAME = root_dir + '/result/{0}_{1}.xlsx'.format(now_time, file_name)\n",
    "    wb = Workbook()\n",
    "    ws = wb.worksheets[0]\n",
    "    header = ['작성자', '닉네임', '본문/댓글', '카테고리명', '제목명', '내용', '게시글 URL', '작성 시각', '수집 시각']\n",
    "    ws.column_dimensions['A'].width = 25\n",
    "    ws.column_dimensions['B'].width = 25\n",
    "    ws.column_dimensions['C'].width = 10\n",
    "    ws.column_dimensions['D'].width = 20\n",
    "    ws.column_dimensions['E'].width = 50\n",
    "    ws.column_dimensions['F'].width = 70\n",
    "    ws.column_dimensions['G'].width = 55\n",
    "    ws.column_dimensions['H'].width = 20\n",
    "    ws.column_dimensions['I'].width = 20\n",
    "    ws.append(header)\n",
    "    wb.save(FILENAME)\n",
    "    return FILENAME\n",
    "\n",
    "\n",
    "def make_excel(data, FILENAME):\n",
    "    wb = load_workbook(FILENAME)\n",
    "    ws = wb.worksheets[0]\n",
    "\n",
    "    for iter in data:\n",
    "        if iter['ok'] == 'error':\n",
    "            continue\n",
    "        email = '{}@naver.com'.format(iter['author_id'])\n",
    "        temp_list = [email, iter['nickname'], '본문', iter['category'], iter['title'], iter['content'], iter['url'], iter['time'], iter['timestamp']]\n",
    "        try:\n",
    "            ws.append(temp_list)\n",
    "        except:\n",
    "            ws.append([email, iter['nickname'], '본문', iter['category'], '', '', iter['url'], iter['time'], iter['timestamp']])\n",
    "        if iter['comment_counts'] > 0:\n",
    "            for comm in iter['comments']:\n",
    "                email2 = '{}@naver.com'.format(comm['author_id'])\n",
    "                try:\n",
    "                    temp_list2 = [email2, comm['nickname'], '', iter['category'], iter['title'], comm['comment'], iter['url'], comm['time'], iter['timestamp']]\n",
    "                    ws.append(temp_list2)\n",
    "                except:\n",
    "                    temp_list2 = [email2, comm['nickname'], '', iter['category'], '', '', iter['url'], comm['time'], iter['timestamp']]\n",
    "                    ws.append(temp_list2)\n",
    "\n",
    "    wb.save(FILENAME)\n",
    "\n",
    "\n",
    "def login(id, pw):\n",
    "    # MAX_TRY == 0일 경우 바로 수동 로그인\n",
    "    if conf['max_try'] == 0:\n",
    "        driver.get('https://nid.naver.com/nidlogin.login')\n",
    "        driver.implicitly_wait(3)\n",
    "        alert(700, 550)\n",
    "        logger.info('[LOGIN] 직접 로그인 해주세요.')\n",
    "        while True:\n",
    "            login_chk = input('완료 시 입력하세요 (완료: 1) :: ')\n",
    "            if login_chk == '1':\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"[LOGIN] 다시 입력해주세요.\")\n",
    "    else:\n",
    "        logger.info(\"[ATTEMPT] 로그인 시도 중...\")\n",
    "        for i in tqdm(range(1, conf['max_try'] + 1)):\n",
    "            driver.get('https://nid.naver.com/nidlogin.login')\n",
    "            driver.implicitly_wait(3)\n",
    "            driver.find_element_by_xpath('//*[@id=\"id\"]').send_keys(id.strip())\n",
    "            driver.implicitly_wait(3)\n",
    "            driver.find_element_by_xpath('//*[@id=\"pw\"]').send_keys(pw.strip())\n",
    "            driver.implicitly_wait(3)\n",
    "            driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/input').click()\n",
    "            driver.implicitly_wait(3)\n",
    "\n",
    "            if driver.current_url == 'https://www.naver.com/':\n",
    "                break\n",
    "\n",
    "        if driver.current_url == 'https://www.naver.com/':\n",
    "            logger.info(\"[COMPLETE] 로그인 성공\")\n",
    "        else:\n",
    "            logger.info(\"[DEBUG] 직접 로그인해주세요...\")\n",
    "            alert(700, 550)\n",
    "            while True:\n",
    "                login_chk = input('완료 시 입력하세요 (완료: 1) :: ')\n",
    "                if login_chk == '1':\n",
    "                    break\n",
    "                else:\n",
    "                    logger.info(\"[DEBUG] 다시 입력해주세요...\")\n",
    "\n",
    "\n",
    "def get_club_id(cafe_url):\n",
    "    driver.get(cafe_url)\n",
    "    driver.implicitly_wait(3)\n",
    "    bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    return str(bs.find('input', {'name': 'clubid'})['value'])\n",
    "    # html = requests.get(cafe_url, headers=headers).text\n",
    "    # bs = BeautifulSoup(html, 'lxml')\n",
    "    # return str(bs.find('input', {'name': 'clubid'})['value'])\n",
    "\n",
    "\n",
    "def get_page_len(club_id, keyword, count=1, crawl_all=False, cafe_url=''):\n",
    "    page_len = count\n",
    "\n",
    "    # 페이지 제한 있을 경우 페이지 넘으면 리턴\n",
    "    if (conf['max_page'] != 0) and (page_len > conf['max_page']):\n",
    "        return conf['max_page']\n",
    "\n",
    "    # 전체글 보기 url 설정\n",
    "    if crawl_all:\n",
    "        url = cafe_url + '?iframe_url=/ArticleList.nhn%3Fsearch.clubid={}%26search.boardtype=L%26search.page={}&userDisplay=50'.format(club_id, str(page_len))\n",
    "    # 키워드 검색 url 설정\n",
    "    else:\n",
    "        encoded_keyword = str(str(keyword).encode('euc-kr'))[1:].replace('\\\\x', '%')\n",
    "        url = 'https://cafe.naver.com/ArticleSearchList.nhn?search.clubid={0}&search.searchdate=all&search.searchBy=1&search.query={1}&search.sortBy=date&userDisplay=50&search.media=0&search.option=0&search.page={2}'.format(\n",
    "            club_id, encoded_keyword, str(page_len))\n",
    "\n",
    "    # 검색\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.switch_to.frame(driver.find_element_by_id('cafe_main'))\n",
    "    html = driver.page_source\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # 검색 페이지 수 수집\n",
    "    try:\n",
    "        page_tds = bs.find('div', class_='prev-next').find('table').find('tr').find_all('td')\n",
    "        for td in page_tds:\n",
    "            if td.has_attr('class'):\n",
    "                if 'pgL' in td['class']:\n",
    "                    continue\n",
    "                elif 'pgR' in td['class']:\n",
    "                    page_len = get_page_len(club_id, keyword, count=page_len + 1, crawl_all=crawl_all, cafe_url=cafe_url)\n",
    "            else:\n",
    "                page_len += 1\n",
    "    except AttributeError:\n",
    "        page_tds = bs.find('div', class_='prev-next').find_all('a')\n",
    "        for td in page_tds:\n",
    "            if td.has_attr('class'):\n",
    "                if 'pgL' in td['class']:\n",
    "                    continue\n",
    "                elif 'pgR' in td['class']:\n",
    "                    page_len = get_page_len(club_id, keyword, count=page_len + 1, crawl_all=crawl_all, cafe_url=cafe_url)\n",
    "            else:\n",
    "                page_len += 1\n",
    "\n",
    "    return page_len\n",
    "\n",
    "\n",
    "def get_posts(club_id, keyword, pages, history_ids=list(), crawl_all=False, cafe_url=''):\n",
    "    result = list()\n",
    "\n",
    "    for page in tqdm(range(1, pages + 1)):\n",
    "        # 전체 글 보기\n",
    "        if crawl_all:\n",
    "            url = cafe_url + '?iframe_url=/ArticleList.nhn%3Fsearch.clubid={}%26search.boardtype=L%26search.page={}&userDisplay=50'.format(club_id, page)\n",
    "        # 키워드 검색 url\n",
    "        else:\n",
    "            encoded_keyword = str(str(keyword).encode('euc-kr'))[1:].replace('\\\\x', '%')\n",
    "            url = 'https://cafe.naver.com/ArticleSearchList.nhn?search.clubid={0}&search.searchdate=all&search.searchBy=1&search.query={1}&search.sortBy=date&userDisplay=50&search.media=0&search.option=0&search.page={2}'.format(club_id, encoded_keyword, str(page))\n",
    "\n",
    "        # 검색\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(3)\n",
    "        driver.switch_to.frame(driver.find_element_by_id('cafe_main'))\n",
    "        html = driver.page_source\n",
    "        bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        try:\n",
    "            trs = bs.find('form', {'name': 'ArticleList'}).find('table').find_all('tr', {'align': 'center'})\n",
    "            for tr in trs:\n",
    "                temp_dict = dict()\n",
    "                temp_dict['post_id'] = tr.find('span', class_='list-count').get_text()\n",
    "                temp_dict['title'] = tr.find('span', class_='aaa').find('a', class_='m-tcol-c').get_text().strip()\n",
    "                temp = tr.find('td', class_='p-nick').find('a').get('onclick')\n",
    "                temp_dict['author_id'] = temp.split(',')[1].replace(\"'\", '').strip()\n",
    "                temp_dict['nickname'] = temp.split(',')[3].replace(\"'\", '').strip()\n",
    "                if temp_dict['title'][0] == '=':\n",
    "                    temp_dict['title'][0] = ''\n",
    "\n",
    "                if temp_dict['post_id'] in history_ids:\n",
    "                    continue\n",
    "                else:\n",
    "                    result.append(temp_dict)\n",
    "        except Exception:\n",
    "            trs = bs.find_all('div', class_='article-board')[1].find('tbody').find_all('tr')\n",
    "            for tr in trs:\n",
    "                try:\n",
    "                    tr.find('a', class_='article').get_text()\n",
    "                except:\n",
    "                    continue\n",
    "                temp_dict = dict()\n",
    "                #temp_dict['title'] = tr.find('div', class_='inner_list').find('a', class_='article').get_text()\n",
    "                temp_dict['title'] = tr.find('a', class_='article').get_text()\n",
    "                temp = tr.find('td', class_='p-nick').find('a').get('onclick')\n",
    "                temp_dict['author_id'] = temp.split(',')[1].replace(\"'\", '').strip()\n",
    "                temp_dict['nickname'] = temp.split(',')[3].replace(\"'\", '').strip()\n",
    "                temp_dict['post_id'] = re.search('articleid=[0-9]+', tr.find('a', class_='article')['href']).group().replace('articleid=', '')\n",
    "\n",
    "                if temp_dict['post_id'] in history_ids:\n",
    "                    continue\n",
    "                else:\n",
    "                    result.append(temp_dict)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_comments(url, club_id, post_id):\n",
    "    all_comment = []\n",
    "    url_dict = dict()\n",
    "    url_dict['url'] = url\n",
    "\n",
    "    # Make Json URL\n",
    "    try:\n",
    "        article_attr = 'search.clubid={0}&search.menuid=26&search.articleid={1}&search.lastpageview=true&lcs=Y'.format(club_id, post_id)\n",
    "        json_chk_url = 'https://cafe.naver.com/CommentView.nhn?' + article_attr\n",
    "\n",
    "        temp_data = requests.get(json_chk_url).text\n",
    "\n",
    "        try:\n",
    "            comment_data = json.loads(temp_data)\n",
    "            url_chk = 0\n",
    "        except:\n",
    "            driver.get(json_chk_url)\n",
    "            bs4 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            comment_data = json.loads(bs4.get_text())\n",
    "            url_chk = 1\n",
    "\n",
    "        # Count comment pages\n",
    "        total = comment_data['result']['totalCount']\n",
    "        cnt_per_page = comment_data['result']['countPerPage']\n",
    "        page_count = total / cnt_per_page\n",
    "        if total % cnt_per_page != 0:\n",
    "            page_count += 1\n",
    "        page_count = int(page_count)\n",
    "\n",
    "        json_url_list = []\n",
    "        for num in range(1, page_count + 1):\n",
    "            json_url_list.append(\n",
    "                'https://cafe.naver.com/CommentView.nhn?search.page={}&'.format(num) + article_attr)\n",
    "\n",
    "    except UnexpectedAlertPresentException:\n",
    "        alert = driver.switch_to_alert()\n",
    "        logger.info(\"[PASS] ({}) {}\".format(url.strip(), alert.text))\n",
    "        alert.accept()\n",
    "        driver.implicitly_wait(3)\n",
    "        time.sleep(2)\n",
    "        return list()\n",
    "\n",
    "    # Get comment\n",
    "    comment_list = []\n",
    "    for json_url in json_url_list:\n",
    "        comment_data = {}\n",
    "        if url_chk == 0:\n",
    "            temp_data = requests.get(json_url).text\n",
    "            comment_data = json.loads(temp_data)\n",
    "        elif url_chk == 1:\n",
    "            driver.get(json_url)\n",
    "            bs4 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            comment_data = json.loads(bs4.get_text())\n",
    "\n",
    "        for comment in comment_data['result']['list']:\n",
    "            temp_comment = {}\n",
    "            if comment['deleted']:\n",
    "                continue\n",
    "            if comment['articleWriter']:\n",
    "                continue\n",
    "            temp_comment['author_id'] = comment['writerid']\n",
    "            if temp_comment['author_id'] in blacklist:\n",
    "                continue\n",
    "            temp_comment['nickname'] = comment['writernick']\n",
    "            temp_comment['time'] = comment['writedt']\n",
    "            temp_comment['comment'] = comment['content'].replace('=', '')\n",
    "\n",
    "            # 수집 제외 문구 있을 경우 무시\n",
    "            if pat_check(ban_list['comment'], temp_comment['comment']):\n",
    "                continue\n",
    "            comment_list.append(temp_comment)\n",
    "\n",
    "    url_dict['comments'] = comment_list\n",
    "    return url_dict['comments']\n",
    "\n",
    "\n",
    "def get_post_info(cafe_url, club_id, post, blacklist=None):\n",
    "    result = dict()\n",
    "    url = 'https://cafe.naver.com/ArticleRead.nhn?clubid={0}&page=10&userDisplay=50&inCafeSearch=true&searchBy=1&query=vs&includeAll=&exclude=&include=&exact=&searchdate=all&media=0&sortBy=date&articleid={1}&referrerAllArticles=true'.format(\n",
    "        club_id, str(post['post_id']))\n",
    "    try:\n",
    "        html = requests.get(url, headers=headers).text\n",
    "        bs = BeautifulSoup(html, 'lxml')\n",
    "        temp = bs.find('td', class_='p-nick').find('a').get('onclick')\n",
    "    except Exception:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(3)\n",
    "            try:\n",
    "                driver.switch_to.frame(driver.find_element_by_id('cafe_main'))\n",
    "                bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                temp = bs.find('td', class_='p-nick').find('a').get('onclick')\n",
    "            except:  # 권한 문제\n",
    "                result['author_id'] = post['author_id']\n",
    "                result['nickname'] = post['nickname']\n",
    "                result['title'] = post['title']\n",
    "                result['time'] = ''\n",
    "                result['content'] = ''\n",
    "                result['category'] = ''\n",
    "                result['url'] = cafe_url + '/' + str(post['post_id'])\n",
    "                result['comment_counts'] = 0\n",
    "                result['comments'] = list()\n",
    "                result['timestamp'] = str(_get_now_time())\n",
    "                result['ok'] = 'success'\n",
    "                return result\n",
    "        except UnexpectedAlertPresentException:\n",
    "            alert = driver.switch_to_alert()\n",
    "            logger.info(\"[PASS] ({}) {}\".format(url.strip(), alert.text))\n",
    "            alert.accept()\n",
    "            driver.implicitly_wait(3)\n",
    "            time.sleep(2)\n",
    "            return {'ok': 'error'}\n",
    "\n",
    "    # 작성자 정보\n",
    "    result['author_id'] = temp.split(',')[1].replace(\"'\", '').strip()\n",
    "    result['nickname'] = temp.split(',')[3].replace(\"'\", '').strip()\n",
    "\n",
    "    # 블랙리스트 포함 여부 확인\n",
    "    if result['author_id'] in blacklist:\n",
    "        result['ok'] = 'error'\n",
    "        return result\n",
    "\n",
    "    # 글 정보 (제목 & 카테고리, 작성시간)\n",
    "    temp2 = bs.find('div', class_='tit-box').find_all('table')\n",
    "    result['title'] = temp2[0].find('span', class_='b m-tcol-c').get_text().replace('=', '').strip()\n",
    "    result['category'] = temp2[0].find('a', class_='m-tcol-c').get_text().strip()\n",
    "    result['time'] = temp2[1].find('td', class_='date').get_text().strip()\n",
    "\n",
    "    # 글 내용\n",
    "    result['url'] = cafe_url + '/' + str(post['post_id'])\n",
    "    if bs.find('div', class_='trading_area') is not None:\n",
    "        result['content'] = ''\n",
    "    else:\n",
    "        result['content'] = bs.find('div', class_='tbody m-tcol-c').get_text().replace('\\xa0', '').replace('=', '').strip()\n",
    "\n",
    "    # 글 내용에 수집 제외 문구 있으면 False 리턴\n",
    "    if pat_check(ban_list['main'], result['content']):\n",
    "        return False\n",
    "\n",
    "    # 댓글 수집\n",
    "    result['comments'] = get_comments(result['url'], club_id, post['post_id'])\n",
    "    result['comment_counts'] = len(result['comments'])\n",
    "\n",
    "    # 기타 수집용 정보\n",
    "    result['timestamp'] = str(_get_now_time())\n",
    "    result['ok'] = 'success'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _get_history(club_id):\n",
    "    file_name = root_dir + '/history/{0}.json'.format(club_id)\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            return json.loads(f.read())\n",
    "    except:\n",
    "        return list()\n",
    "\n",
    "\n",
    "def _make_history(club_id, post_ids):\n",
    "    file_name = root_dir + '/history/{0}.json'.format(club_id)\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(post_ids, f)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) > 2:\n",
    "        root_dir = sys.argv[1]\n",
    "        setting_dir = sys.argv[2]\n",
    "    else:\n",
    "        root_dir = '.'\n",
    "        setting_dir = '.'\n",
    "\n",
    "    if not os.path.exists(root_dir + '/setting'):\n",
    "        os.mkdir(root_dir + '/setting')\n",
    "    if not os.path.exists(root_dir + '/result'):\n",
    "        os.mkdir(root_dir + '/result')\n",
    "    if not os.path.exists(root_dir + '/history'):\n",
    "        os.mkdir(root_dir + '/history')\n",
    "\n",
    "    driver_loc = root_dir + '/chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_loc)\n",
    "\n",
    "    # LOGGER\n",
    "    logger = logging.getLogger('notice')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('[SYSTEM] %(asctime)s :: %(message)s')\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    streamHandler.setFormatter(formatter)\n",
    "    logger.addHandler(streamHandler)\n",
    "\n",
    "    # DEBUG\n",
    "    logger.info('[SYSTEM] 파일 경로 : ' + str(root_dir))\n",
    "    logger.info('[SYSTEM] 드라이버 경로 : ' + str(driver_loc))\n",
    "    logger.info('[SYSTEM] 수집 제외 문구 (본문) 파일 경로 : ' + str(setting_dir + '/ban_list_main.txt'))\n",
    "    logger.info('[SYSTEM] 수집 제외 문구 (댓글) 파일 경로 : ' + str(setting_dir + '/ban_list_comment.txt'))\n",
    "    logger.info('[SYSTEM] 블랙리스트 파일 경로 : ' + str(setting_dir + '/blacklist.txt'))\n",
    "    logger.info('[SYSTEM] 프로그램 세팅 파일 경로 : ' + str(setting_dir + '/program_setting.txt'))\n",
    "\n",
    "    # Load Setting\n",
    "    load_setting()\n",
    "\n",
    "    # Main loop\n",
    "    idx = 1\n",
    "    result_data = list()\n",
    "    posts_list = list()\n",
    "    prev_id = ''\n",
    "    for data in input_data:\n",
    "        result_data.clear()\n",
    "        posts_list.clear()\n",
    "\n",
    "        # 새로 로그인 할 아이디가 같다면\n",
    "        if prev_id != data['id']:\n",
    "            login(data['id'], data['pw'])\n",
    "            prev_id = data['id']\n",
    "        else:\n",
    "            prev_id = data['id']\n",
    "\n",
    "        try:\n",
    "            club_id = get_club_id(data['url'])\n",
    "        except Exception as exc:\n",
    "            try:\n",
    "                club_id = get_club_id(data['url'])\n",
    "            except Exception as exc:\n",
    "                logger.info('[SYSTEM] cafe 수집 불가 : {}'.format(data['url']))\n",
    "                continue\n",
    "                \n",
    "        logger.info('[SYSTEM] 현재 {}번째 cafe : {}'.format(idx, data['url']))\n",
    "        history = _get_history(club_id)\n",
    "        logger.info('[SYSTEM] History 로딩 완료.')\n",
    "        excel_name = set_excel(data['excel_name'])\n",
    "        logger.info('[SYSTEM] 엑셀 파일 설정 완료.')\n",
    "\n",
    "        for keyword in data['keywords']:\n",
    "            result_data.clear()\n",
    "            logger.info('[SYSTEM] 게시글 목록 수집 시작.')\n",
    "            try:\n",
    "                page_len = get_page_len(club_id, keyword, crawl_all=data['all_crawl'], cafe_url=data['url'])\n",
    "                # 최대 페이지 수 확인\n",
    "                if conf['max_page'] == 0:\n",
    "                    logger.info('[SYSTEM] 검색 페이지 수 : {}'.format(str(page_len)))\n",
    "                else:\n",
    "                    if page_len > conf['max_page']:\n",
    "                        page_len = conf['max_page']\n",
    "                        logger.info('[SYSTEM] 검색 페이지 수 : {} (최대 페이지 제한)'.format(str(page_len)))\n",
    "                posts = get_posts(club_id, keyword, page_len, history, crawl_all=data['all_crawl'], cafe_url=data['url'])\n",
    "                post_id_list = list()\n",
    "                for post in posts:\n",
    "                    # 제목에 수집 제외 문구 포함되었으면 무시.\n",
    "                    if pat_check(ban_list['main'], post['title']):\n",
    "                        continue\n",
    "                    post_id_list.append(post['post_id'])\n",
    "                posts_list.extend(post_id_list)\n",
    "            except Exception as exc:\n",
    "                logger.info('[ERROR] 게시글 목록 수집 중 에러 : ' + str(exc))\n",
    "                continue\n",
    "\n",
    "            logger.info('[SYSTEM] 게시글 상세 데이터 수집 시작.')\n",
    "            cnt = 0\n",
    "            for post in tqdm(posts):\n",
    "                if cnt > 50:\n",
    "                    make_excel(result_data, excel_name)\n",
    "                    result_data.clear()\n",
    "                    cnt = 0\n",
    "                try:\n",
    "                    cnt += 1\n",
    "                    post_temp = get_post_info(data['url'], club_id, post, blacklist=blacklist)\n",
    "                    if post_temp == False:\n",
    "                        continue\n",
    "                    result_data.append(post_temp)\n",
    "                except UnexpectedAlertPresentException:\n",
    "                    alert = driver.switch_to_alert()\n",
    "                    alert.accept()\n",
    "                    driver.implicitly_wait(3)\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                except Exception as exc:\n",
    "                    logger.info('[ERROR] 게시글 내용 수집 중 에러 : ' + str(exc))\n",
    "                    continue\n",
    "            make_excel(result_data, excel_name)\n",
    "            result_data.clear()\n",
    "        logger.info('[SYSTEM] 엑셀 파일 생성 완료.')\n",
    "        posts_list.extend(history)\n",
    "        _make_history(club_id, posts_list)\n",
    "        logger.info('[SYSTEM] 히스토리 파일 생성 완료.')\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[1]/td[2]/span/span/a\"}\n  (Session info: chrome=80.0.3987.87)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-4b162eff53bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[1]/td[2]/span/span/a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mres_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexttext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[1;34m(self, xpath)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//div/td[1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[1]/td[2]/span/span/a\"}\n  (Session info: chrome=80.0.3987.87)\n"
     ]
    }
   ],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"main-area\"]/div[6]/form/table/tbody/tr[1]/td[2]/span/span/a').click()\n",
    "\n",
    "res_list = []\n",
    "\n",
    "for text in texttext:\n",
    "    driver.get(text)\n",
    "    \n",
    "    title = soup.select('div.a')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-10-3de84345dac3>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-3de84345dac3>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    count = i\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "페 목록 출력 페이지 지정, 옵션 설정\n",
    "pages = 2\n",
    "count = 0\n",
    "sdata = [] #DataFrame 전환 리스트 To CSV파일\n",
    "\n",
    "for i in range(1, pages + 1):  #아래 첫번째 들여쓰기\n",
    "count = i\n",
    "stitle = [] #게시글 제목\n",
    "snickname = [] #카페 닉네임\n",
    "surl = [] #게시글 링크\n",
    "scontent = [] #게시글 내용\n",
    "naverIDs = [] #게시글 사용자정보\n",
    "url = board_url + str(i)\n",
    "driver.get(url)\n",
    "\n",
    "iframe = driver.find_element_by_id('cafe_main')\n",
    "driver.switch_to_frame(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = driver.find_elements_by_css_selector('body > a')\n",
    "article_urls = [ i.get_attribute('href') for i in article_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-a8594e233c00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# time.sleep 작업도 필요하다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# 결과 데이터프레임화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mcafe_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m# csv파일로 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mcafe_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cafe_crawling.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "\n",
    "for article in article_urls:\n",
    "    driver.get(article)\n",
    "    \n",
    "    # article도 switch_to.frame이 필수\n",
    "    driver.switch_to.frame('cafe_main')\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # 게시글에서 제목 추출\n",
    "    title = soup.select('div.tit-box')[0].get_text()\n",
    "    \n",
    "    \n",
    "    # 내용을 하나의 텍스트로 만든다. (띄어쓰기 단위)\n",
    "    content_tags = soup.select('#tbody')[0].select('p')\n",
    "    content = ' '.join([ tags.get_text() for tags in content_tags ])\n",
    "    \n",
    "    \n",
    "    # dict형태로 만들어 결과 list에 저장\n",
    "    res_list.append({'title' : title, 'content' : content})\n",
    "    \n",
    "    \n",
    "    # time.sleep 작업도 필요하다.\n",
    "# 결과 데이터프레임화\n",
    "cafe_df = pd.DataFrame(res_list)\n",
    "# csv파일로 추출\n",
    "cafe_df.to_csv('cafe_crawling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pattern_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-05f099205c11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpattern_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpat_transform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat_find\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pattern_search'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"gm-tcol-t\" href=\"#\" onclick=\"toggleMenuGroup('group155');return false;\" title=\"국  가  정  보\">국  가  정  보</a>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://cafe.naver.com/firenze.cafe\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('#cafe-menu > div.box-g-m > div:nth-child(16) > h3 > a')\n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국  가  정  보']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles = [title.get_text() for title in titles]\n",
    "music_titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
