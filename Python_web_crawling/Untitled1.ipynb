{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고코드\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import winsound\n",
    "import requests\n",
    "import configparser\n",
    "import urllib.parse\n",
    "import logging.handlers\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from openpyxl import *\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import UnexpectedAlertPresentException\n",
    "\n",
    "from pattern_search import pat_transform, pat_find, pat_check\n",
    "\n",
    "\n",
    "# 전역변수\n",
    "root_dir = ''\n",
    "setting_dir = ''\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'}\n",
    "driver_loc = '/chromedriver.exe'\n",
    "input_data = list()\n",
    "blacklist = list()\n",
    "conf = {\n",
    "    'max_try': 50,   # 로그인 최대 시도 횟수. (0이면 바로 수동로그인)\n",
    "    'alert': 3,     # 비프음 횟수. (0이면 비프 알림 없음)\n",
    "    'max_page': 0   # 각 url당 수집할 최대 페이지. (0이면 모두 수집)\n",
    "}\n",
    "ban_list = {\n",
    "    'main': list(), # 본문 수집 제외 문구 리스트\n",
    "    'comment': list()   # 댓글 수집 제외 문구 리스트\n",
    "}\n",
    "\n",
    "\n",
    "def alert(a, b):\n",
    "    for i in range(1, conf['alert'] + 1):\n",
    "        winsound.Beep(a, 500)\n",
    "        winsound.Beep(b, 500)\n",
    "        winsound.Beep(a, 500)\n",
    "        winsound.Beep(b, 500)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "def load_setting():\n",
    "    path_dir = root_dir + '/setting'\n",
    "    file_list = os.listdir(path_dir)\n",
    "    file_list.sort()\n",
    "\n",
    "    # 수집 제외 문구 리스트 ( 본문 )\n",
    "    with open(setting_dir + '/ban_list_main.txt', 'r') as bf:\n",
    "        for line in bf.readlines():\n",
    "            ban_list['main'].append(pat_transform(line.strip()))\n",
    "\n",
    "    # 수집 제외 문구 리스트 ( 댓글 )\n",
    "    with open(setting_dir + '/ban_list_comment.txt', 'r') as bf:\n",
    "        for line in bf.readlines():\n",
    "            ban_list['comment'].append(pat_transform(line.strip()))\n",
    "\n",
    "    # 블랙 리스트\n",
    "    with open(setting_dir + '/blacklist.txt', 'r') as bf:\n",
    "        for line in bf.readlines():\n",
    "            blacklist.append(line.strip().replace('@naver.com', ''))\n",
    "    \n",
    "    # 프로그램 세팅 파일\n",
    "    with open(setting_dir + '/program_setting.txt', 'r') as bf:\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(setting_dir + '/program_setting.txt')\n",
    "        conf['max_try'] = int(config['LOGIN']['MAX_TRY'])\n",
    "        conf['alert'] = int(config['LOGIN']['BEEP_ALERT'])\n",
    "        conf['max_page'] = int(config['PROGRAM']['MAX_PAGE'])\n",
    "        logger.info('[SETTING] 로그인 최대 시도 횟수 : {}'.format(conf['max_try']))\n",
    "        logger.info('[SETTING] 자동 로그인 실패 시 알림 횟수 : {}'.format(conf['alert']))\n",
    "        logger.info('[SETTING] 각 카페 당 최대 수집 페이지 수 : {}'.format(conf['max_page']))\n",
    "\n",
    "\n",
    "    for iter in file_list:\n",
    "        file_name = '{0}/{1}'.format(path_dir, iter)\n",
    "\n",
    "        with open(file_name, 'r') as f:\n",
    "            temp_dict = {'keywords': list()}\n",
    "\n",
    "            lines = f.readlines()\n",
    "            now = 'url'\n",
    "            for line in lines:\n",
    "                if line[0] == '#':\n",
    "                    if 'url' in line.strip():\n",
    "                        now = 'url'\n",
    "                    elif 'id / pw' in line.strip():\n",
    "                        now = 'account'\n",
    "                    elif 'keyword' in line.strip():\n",
    "                        now = 'keyword'\n",
    "                    elif 'excel' in line.strip():\n",
    "                        now = 'excel'\n",
    "                    elif '전체' in line.strip():\n",
    "                        now = 'all_crawl'\n",
    "                else:\n",
    "                    if line.strip() == '':\n",
    "                        continue\n",
    "\n",
    "                    if now == 'url':\n",
    "                        temp_dict['url'] = line.strip()\n",
    "                    elif now == 'account':\n",
    "                        temp_dict['id'] = line.split(' ')[0].strip()\n",
    "                        temp_dict['pw'] = line.split(' ')[1].strip()\n",
    "                    elif now == 'keyword':\n",
    "                        temp_dict['keywords'].append(line.strip())\n",
    "                    elif now == 'excel':\n",
    "                        temp_dict['excel_name'] = line.strip()\n",
    "                    elif now == 'all_crawl':\n",
    "                        string = line.strip()\n",
    "                        if string in ['True', 'true']:\n",
    "                            temp_dict['all_crawl'] = True\n",
    "                        else:\n",
    "                            temp_dict['all_crawl'] = False\n",
    "            input_data.append(temp_dict)\n",
    "\n",
    "\n",
    "def _get_now_time():\n",
    "    now = time.localtime()\n",
    "    s = \"{0}.{1:0>2}.{2:0>2}. {3:0>2}:{4:0>2}:{5:0>2}\".format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "    return s\n",
    "\n",
    "\n",
    "def set_excel(file_name):\n",
    "    now_time = time.strftime('%Y%m%d%H%M%S', time.localtime())\n",
    "    FILENAME = root_dir + '/result/{0}_{1}.xlsx'.format(now_time, file_name)\n",
    "    wb = Workbook()\n",
    "    ws = wb.worksheets[0]\n",
    "    header = ['작성자', '닉네임', '본문/댓글', '카테고리명', '제목명', '내용', '게시글 URL', '작성 시각', '수집 시각']\n",
    "    ws.column_dimensions['A'].width = 25\n",
    "    ws.column_dimensions['B'].width = 25\n",
    "    ws.column_dimensions['C'].width = 10\n",
    "    ws.column_dimensions['D'].width = 20\n",
    "    ws.column_dimensions['E'].width = 50\n",
    "    ws.column_dimensions['F'].width = 70\n",
    "    ws.column_dimensions['G'].width = 55\n",
    "    ws.column_dimensions['H'].width = 20\n",
    "    ws.column_dimensions['I'].width = 20\n",
    "    ws.append(header)\n",
    "    wb.save(FILENAME)\n",
    "    return FILENAME\n",
    "\n",
    "\n",
    "def make_excel(data, FILENAME):\n",
    "    wb = load_workbook(FILENAME)\n",
    "    ws = wb.worksheets[0]\n",
    "\n",
    "    for iter in data:\n",
    "        if iter['ok'] == 'error':\n",
    "            continue\n",
    "        email = '{}@naver.com'.format(iter['author_id'])\n",
    "        temp_list = [email, iter['nickname'], '본문', iter['category'], iter['title'], iter['content'], iter['url'], iter['time'], iter['timestamp']]\n",
    "        try:\n",
    "            ws.append(temp_list)\n",
    "        except:\n",
    "            ws.append([email, iter['nickname'], '본문', iter['category'], '', '', iter['url'], iter['time'], iter['timestamp']])\n",
    "        if iter['comment_counts'] > 0:\n",
    "            for comm in iter['comments']:\n",
    "                email2 = '{}@naver.com'.format(comm['author_id'])\n",
    "                try:\n",
    "                    temp_list2 = [email2, comm['nickname'], '', iter['category'], iter['title'], comm['comment'], iter['url'], comm['time'], iter['timestamp']]\n",
    "                    ws.append(temp_list2)\n",
    "                except:\n",
    "                    temp_list2 = [email2, comm['nickname'], '', iter['category'], '', '', iter['url'], comm['time'], iter['timestamp']]\n",
    "                    ws.append(temp_list2)\n",
    "\n",
    "    wb.save(FILENAME)\n",
    "\n",
    "\n",
    "def login(id, pw):\n",
    "    # MAX_TRY == 0일 경우 바로 수동 로그인\n",
    "    if conf['max_try'] == 0:\n",
    "        driver.get('https://nid.naver.com/nidlogin.login')\n",
    "        driver.implicitly_wait(3)\n",
    "        alert(700, 550)\n",
    "        logger.info('[LOGIN] 직접 로그인 해주세요.')\n",
    "        while True:\n",
    "            login_chk = input('완료 시 입력하세요 (완료: 1) :: ')\n",
    "            if login_chk == '1':\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"[LOGIN] 다시 입력해주세요.\")\n",
    "    else:\n",
    "        logger.info(\"[ATTEMPT] 로그인 시도 중...\")\n",
    "        for i in tqdm(range(1, conf['max_try'] + 1)):\n",
    "            driver.get('https://nid.naver.com/nidlogin.login')\n",
    "            driver.implicitly_wait(3)\n",
    "            driver.find_element_by_xpath('//*[@id=\"id\"]').send_keys(id.strip())\n",
    "            driver.implicitly_wait(3)\n",
    "            driver.find_element_by_xpath('//*[@id=\"pw\"]').send_keys(pw.strip())\n",
    "            driver.implicitly_wait(3)\n",
    "            driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/input').click()\n",
    "            driver.implicitly_wait(3)\n",
    "\n",
    "            if driver.current_url == 'https://www.naver.com/':\n",
    "                break\n",
    "\n",
    "        if driver.current_url == 'https://www.naver.com/':\n",
    "            logger.info(\"[COMPLETE] 로그인 성공\")\n",
    "        else:\n",
    "            logger.info(\"[DEBUG] 직접 로그인해주세요...\")\n",
    "            alert(700, 550)\n",
    "            while True:\n",
    "                login_chk = input('완료 시 입력하세요 (완료: 1) :: ')\n",
    "                if login_chk == '1':\n",
    "                    break\n",
    "                else:\n",
    "                    logger.info(\"[DEBUG] 다시 입력해주세요...\")\n",
    "\n",
    "\n",
    "def get_club_id(cafe_url):\n",
    "    driver.get(cafe_url)\n",
    "    driver.implicitly_wait(3)\n",
    "    bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    return str(bs.find('input', {'name': 'clubid'})['value'])\n",
    "    # html = requests.get(cafe_url, headers=headers).text\n",
    "    # bs = BeautifulSoup(html, 'lxml')\n",
    "    # return str(bs.find('input', {'name': 'clubid'})['value'])\n",
    "\n",
    "\n",
    "def get_page_len(club_id, keyword, count=1, crawl_all=False, cafe_url=''):\n",
    "    page_len = count\n",
    "\n",
    "    # 페이지 제한 있을 경우 페이지 넘으면 리턴\n",
    "    if (conf['max_page'] != 0) and (page_len > conf['max_page']):\n",
    "        return conf['max_page']\n",
    "\n",
    "    # 전체글 보기 url 설정\n",
    "    if crawl_all:\n",
    "        url = cafe_url + '?iframe_url=/ArticleList.nhn%3Fsearch.clubid={}%26search.boardtype=L%26search.page={}&userDisplay=50'.format(club_id, str(page_len))\n",
    "    # 키워드 검색 url 설정\n",
    "    else:\n",
    "        encoded_keyword = str(str(keyword).encode('euc-kr'))[1:].replace('\\\\x', '%')\n",
    "        url = 'https://cafe.naver.com/ArticleSearchList.nhn?search.clubid={0}&search.searchdate=all&search.searchBy=1&search.query={1}&search.sortBy=date&userDisplay=50&search.media=0&search.option=0&search.page={2}'.format(\n",
    "            club_id, encoded_keyword, str(page_len))\n",
    "\n",
    "    # 검색\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.switch_to.frame(driver.find_element_by_id('cafe_main'))\n",
    "    html = driver.page_source\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # 검색 페이지 수 수집\n",
    "    try:\n",
    "        page_tds = bs.find('div', class_='prev-next').find('table').find('tr').find_all('td')\n",
    "        for td in page_tds:\n",
    "            if td.has_attr('class'):\n",
    "                if 'pgL' in td['class']:\n",
    "                    continue\n",
    "                elif 'pgR' in td['class']:\n",
    "                    page_len = get_page_len(club_id, keyword, count=page_len + 1, crawl_all=crawl_all, cafe_url=cafe_url)\n",
    "            else:\n",
    "                page_len += 1\n",
    "    except AttributeError:\n",
    "        page_tds = bs.find('div', class_='prev-next').find_all('a')\n",
    "        for td in page_tds:\n",
    "            if td.has_attr('class'):\n",
    "                if 'pgL' in td['class']:\n",
    "                    continue\n",
    "                elif 'pgR' in td['class']:\n",
    "                    page_len = get_page_len(club_id, keyword, count=page_len + 1, crawl_all=crawl_all, cafe_url=cafe_url)\n",
    "            else:\n",
    "                page_len += 1\n",
    "\n",
    "    return page_len\n",
    "\n",
    "\n",
    "def get_posts(club_id, keyword, pages, history_ids=list(), crawl_all=False, cafe_url=''):\n",
    "    result = list()\n",
    "\n",
    "    for page in tqdm(range(1, pages + 1)):\n",
    "        # 전체 글 보기\n",
    "        if crawl_all:\n",
    "            url = cafe_url + '?iframe_url=/ArticleList.nhn%3Fsearch.clubid={}%26search.boardtype=L%26search.page={}&userDisplay=50'.format(club_id, page)\n",
    "        # 키워드 검색 url\n",
    "        else:\n",
    "            encoded_keyword = str(str(keyword).encode('euc-kr'))[1:].replace('\\\\x', '%')\n",
    "            url = 'https://cafe.naver.com/ArticleSearchList.nhn?search.clubid={0}&search.searchdate=all&search.searchBy=1&search.query={1}&search.sortBy=date&userDisplay=50&search.media=0&search.option=0&search.page={2}'.format(club_id, encoded_keyword, str(page))\n",
    "\n",
    "        # 검색\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(3)\n",
    "        driver.switch_to.frame(driver.find_element_by_id('cafe_main'))\n",
    "        html = driver.page_source\n",
    "        bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        try:\n",
    "            trs = bs.find('form', {'name': 'ArticleList'}).find('table').find_all('tr', {'align': 'center'})\n",
    "            for tr in trs:\n",
    "                temp_dict = dict()\n",
    "                temp_dict['post_id'] = tr.find('span', class_='list-count').get_text()\n",
    "                temp_dict['title'] = tr.find('span', class_='aaa').find('a', class_='m-tcol-c').get_text().strip()\n",
    "                temp = tr.find('td', class_='p-nick').find('a').get('onclick')\n",
    "                temp_dict['author_id'] = temp.split(',')[1].replace(\"'\", '').strip()\n",
    "                temp_dict['nickname'] = temp.split(',')[3].replace(\"'\", '').strip()\n",
    "                if temp_dict['title'][0] == '=':\n",
    "                    temp_dict['title'][0] = ''\n",
    "\n",
    "                if temp_dict['post_id'] in history_ids:\n",
    "                    continue\n",
    "                else:\n",
    "                    result.append(temp_dict)\n",
    "        except Exception:\n",
    "            trs = bs.find_all('div', class_='article-board')[1].find('tbody').find_all('tr')\n",
    "            for tr in trs:\n",
    "                try:\n",
    "                    tr.find('a', class_='article').get_text()\n",
    "                except:\n",
    "                    continue\n",
    "                temp_dict = dict()\n",
    "                #temp_dict['title'] = tr.find('div', class_='inner_list').find('a', class_='article').get_text()\n",
    "                temp_dict['title'] = tr.find('a', class_='article').get_text()\n",
    "                temp = tr.find('td', class_='p-nick').find('a').get('onclick')\n",
    "                temp_dict['author_id'] = temp.split(',')[1].replace(\"'\", '').strip()\n",
    "                temp_dict['nickname'] = temp.split(',')[3].replace(\"'\", '').strip()\n",
    "                temp_dict['post_id'] = re.search('articleid=[0-9]+', tr.find('a', class_='article')['href']).group().replace('articleid=', '')\n",
    "\n",
    "                if temp_dict['post_id'] in history_ids:\n",
    "                    continue\n",
    "                else:\n",
    "                    result.append(temp_dict)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_comments(url, club_id, post_id):\n",
    "    all_comment = []\n",
    "    url_dict = dict()\n",
    "    url_dict['url'] = url\n",
    "\n",
    "    # Make Json URL\n",
    "    try:\n",
    "        article_attr = 'search.clubid={0}&search.menuid=26&search.articleid={1}&search.lastpageview=true&lcs=Y'.format(club_id, post_id)\n",
    "        json_chk_url = 'https://cafe.naver.com/CommentView.nhn?' + article_attr\n",
    "\n",
    "        temp_data = requests.get(json_chk_url).text\n",
    "\n",
    "        try:\n",
    "            comment_data = json.loads(temp_data)\n",
    "            url_chk = 0\n",
    "        except:\n",
    "            driver.get(json_chk_url)\n",
    "            bs4 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            comment_data = json.loads(bs4.get_text())\n",
    "            url_chk = 1\n",
    "\n",
    "        # Count comment pages\n",
    "        total = comment_data['result']['totalCount']\n",
    "        cnt_per_page = comment_data['result']['countPerPage']\n",
    "        page_count = total / cnt_per_page\n",
    "        if total % cnt_per_page != 0:\n",
    "            page_count += 1\n",
    "        page_count = int(page_count)\n",
    "\n",
    "        json_url_list = []\n",
    "        for num in range(1, page_count + 1):\n",
    "            json_url_list.append(\n",
    "                'https://cafe.naver.com/CommentView.nhn?search.page={}&'.format(num) + article_attr)\n",
    "\n",
    "    except UnexpectedAlertPresentException:\n",
    "        alert = driver.switch_to_alert()\n",
    "        logger.info(\"[PASS] ({}) {}\".format(url.strip(), alert.text))\n",
    "        alert.accept()\n",
    "        driver.implicitly_wait(3)\n",
    "        time.sleep(2)\n",
    "        return list()\n",
    "\n",
    "    # Get comment\n",
    "    comment_list = []\n",
    "    for json_url in json_url_list:\n",
    "        comment_data = {}\n",
    "        if url_chk == 0:\n",
    "            temp_data = requests.get(json_url).text\n",
    "            comment_data = json.loads(temp_data)\n",
    "        elif url_chk == 1:\n",
    "            driver.get(json_url)\n",
    "            bs4 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            comment_data = json.loads(bs4.get_text())\n",
    "\n",
    "        for comment in comment_data['result']['list']:\n",
    "            temp_comment = {}\n",
    "            if comment['deleted']:\n",
    "                continue\n",
    "            if comment['articleWriter']:\n",
    "                continue\n",
    "            temp_comment['author_id'] = comment['writerid']\n",
    "            if temp_comment['author_id'] in blacklist:\n",
    "                continue\n",
    "            temp_comment['nickname'] = comment['writernick']\n",
    "            temp_comment['time'] = comment['writedt']\n",
    "            temp_comment['comment'] = comment['content'].replace('=', '')\n",
    "\n",
    "            # 수집 제외 문구 있을 경우 무시\n",
    "            if pat_check(ban_list['comment'], temp_comment['comment']):\n",
    "                continue\n",
    "            comment_list.append(temp_comment)\n",
    "\n",
    "    url_dict['comments'] = comment_list\n",
    "    return url_dict['comments']\n",
    "\n",
    "\n",
    "def get_post_info(cafe_url, club_id, post, blacklist=None):\n",
    "    result = dict()\n",
    "    url = 'https://cafe.naver.com/ArticleRead.nhn?clubid={0}&page=10&userDisplay=50&inCafeSearch=true&searchBy=1&query=vs&includeAll=&exclude=&include=&exact=&searchdate=all&media=0&sortBy=date&articleid={1}&referrerAllArticles=true'.format(\n",
    "        club_id, str(post['post_id']))\n",
    "    try:\n",
    "        html = requests.get(url, headers=headers).text\n",
    "        bs = BeautifulSoup(html, 'lxml')\n",
    "        temp = bs.find('td', class_='p-nick').find('a').get('onclick')\n",
    "    except Exception:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(3)\n",
    "            try:\n",
    "                driver.switch_to.frame(driver.find_element_by_id('cafe_main'))\n",
    "                bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                temp = bs.find('td', class_='p-nick').find('a').get('onclick')\n",
    "            except:  # 권한 문제\n",
    "                result['author_id'] = post['author_id']\n",
    "                result['nickname'] = post['nickname']\n",
    "                result['title'] = post['title']\n",
    "                result['time'] = ''\n",
    "                result['content'] = ''\n",
    "                result['category'] = ''\n",
    "                result['url'] = cafe_url + '/' + str(post['post_id'])\n",
    "                result['comment_counts'] = 0\n",
    "                result['comments'] = list()\n",
    "                result['timestamp'] = str(_get_now_time())\n",
    "                result['ok'] = 'success'\n",
    "                return result\n",
    "        except UnexpectedAlertPresentException:\n",
    "            alert = driver.switch_to_alert()\n",
    "            logger.info(\"[PASS] ({}) {}\".format(url.strip(), alert.text))\n",
    "            alert.accept()\n",
    "            driver.implicitly_wait(3)\n",
    "            time.sleep(2)\n",
    "            return {'ok': 'error'}\n",
    "\n",
    "    # 작성자 정보\n",
    "    result['author_id'] = temp.split(',')[1].replace(\"'\", '').strip()\n",
    "    result['nickname'] = temp.split(',')[3].replace(\"'\", '').strip()\n",
    "\n",
    "    # 블랙리스트 포함 여부 확인\n",
    "    if result['author_id'] in blacklist:\n",
    "        result['ok'] = 'error'\n",
    "        return result\n",
    "\n",
    "    # 글 정보 (제목 & 카테고리, 작성시간)\n",
    "    temp2 = bs.find('div', class_='tit-box').find_all('table')\n",
    "    result['title'] = temp2[0].find('span', class_='b m-tcol-c').get_text().replace('=', '').strip()\n",
    "    result['category'] = temp2[0].find('a', class_='m-tcol-c').get_text().strip()\n",
    "    result['time'] = temp2[1].find('td', class_='date').get_text().strip()\n",
    "\n",
    "    # 글 내용\n",
    "    result['url'] = cafe_url + '/' + str(post['post_id'])\n",
    "    if bs.find('div', class_='trading_area') is not None:\n",
    "        result['content'] = ''\n",
    "    else:\n",
    "        result['content'] = bs.find('div', class_='tbody m-tcol-c').get_text().replace('\\xa0', '').replace('=', '').strip()\n",
    "\n",
    "    # 글 내용에 수집 제외 문구 있으면 False 리턴\n",
    "    if pat_check(ban_list['main'], result['content']):\n",
    "        return False\n",
    "\n",
    "    # 댓글 수집\n",
    "    result['comments'] = get_comments(result['url'], club_id, post['post_id'])\n",
    "    result['comment_counts'] = len(result['comments'])\n",
    "\n",
    "    # 기타 수집용 정보\n",
    "    result['timestamp'] = str(_get_now_time())\n",
    "    result['ok'] = 'success'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _get_history(club_id):\n",
    "    file_name = root_dir + '/history/{0}.json'.format(club_id)\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            return json.loads(f.read())\n",
    "    except:\n",
    "        return list()\n",
    "\n",
    "\n",
    "def _make_history(club_id, post_ids):\n",
    "    file_name = root_dir + '/history/{0}.json'.format(club_id)\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(post_ids, f)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) > 2:\n",
    "        root_dir = sys.argv[1]\n",
    "        setting_dir = sys.argv[2]\n",
    "    else:\n",
    "        root_dir = '.'\n",
    "        setting_dir = '.'\n",
    "\n",
    "    if not os.path.exists(root_dir + '/setting'):\n",
    "        os.mkdir(root_dir + '/setting')\n",
    "    if not os.path.exists(root_dir + '/result'):\n",
    "        os.mkdir(root_dir + '/result')\n",
    "    if not os.path.exists(root_dir + '/history'):\n",
    "        os.mkdir(root_dir + '/history')\n",
    "\n",
    "    driver_loc = root_dir + '/chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_loc)\n",
    "\n",
    "    # LOGGER\n",
    "    logger = logging.getLogger('notice')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('[SYSTEM] %(asctime)s :: %(message)s')\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    streamHandler.setFormatter(formatter)\n",
    "    logger.addHandler(streamHandler)\n",
    "\n",
    "    # DEBUG\n",
    "    logger.info('[SYSTEM] 파일 경로 : ' + str(root_dir))\n",
    "    logger.info('[SYSTEM] 드라이버 경로 : ' + str(driver_loc))\n",
    "    logger.info('[SYSTEM] 수집 제외 문구 (본문) 파일 경로 : ' + str(setting_dir + '/ban_list_main.txt'))\n",
    "    logger.info('[SYSTEM] 수집 제외 문구 (댓글) 파일 경로 : ' + str(setting_dir + '/ban_list_comment.txt'))\n",
    "    logger.info('[SYSTEM] 블랙리스트 파일 경로 : ' + str(setting_dir + '/blacklist.txt'))\n",
    "    logger.info('[SYSTEM] 프로그램 세팅 파일 경로 : ' + str(setting_dir + '/program_setting.txt'))\n",
    "\n",
    "    # Load Setting\n",
    "    load_setting()\n",
    "\n",
    "    # Main loop\n",
    "    idx = 1\n",
    "    result_data = list()\n",
    "    posts_list = list()\n",
    "    prev_id = ''\n",
    "    for data in input_data:\n",
    "        result_data.clear()\n",
    "        posts_list.clear()\n",
    "\n",
    "        # 새로 로그인 할 아이디가 같다면\n",
    "        if prev_id != data['id']:\n",
    "            login(data['id'], data['pw'])\n",
    "            prev_id = data['id']\n",
    "        else:\n",
    "            prev_id = data['id']\n",
    "\n",
    "        try:\n",
    "            club_id = get_club_id(data['url'])\n",
    "        except Exception as exc:\n",
    "            try:\n",
    "                club_id = get_club_id(data['url'])\n",
    "            except Exception as exc:\n",
    "                logger.info('[SYSTEM] cafe 수집 불가 : {}'.format(data['url']))\n",
    "                continue\n",
    "                \n",
    "        logger.info('[SYSTEM] 현재 {}번째 cafe : {}'.format(idx, data['url']))\n",
    "        history = _get_history(club_id)\n",
    "        logger.info('[SYSTEM] History 로딩 완료.')\n",
    "        excel_name = set_excel(data['excel_name'])\n",
    "        logger.info('[SYSTEM] 엑셀 파일 설정 완료.')\n",
    "\n",
    "        for keyword in data['keywords']:\n",
    "            result_data.clear()\n",
    "            logger.info('[SYSTEM] 게시글 목록 수집 시작.')\n",
    "            try:\n",
    "                page_len = get_page_len(club_id, keyword, crawl_all=data['all_crawl'], cafe_url=data['url'])\n",
    "                # 최대 페이지 수 확인\n",
    "                if conf['max_page'] == 0:\n",
    "                    logger.info('[SYSTEM] 검색 페이지 수 : {}'.format(str(page_len)))\n",
    "                else:\n",
    "                    if page_len > conf['max_page']:\n",
    "                        page_len = conf['max_page']\n",
    "                        logger.info('[SYSTEM] 검색 페이지 수 : {} (최대 페이지 제한)'.format(str(page_len)))\n",
    "                posts = get_posts(club_id, keyword, page_len, history, crawl_all=data['all_crawl'], cafe_url=data['url'])\n",
    "                post_id_list = list()\n",
    "                for post in posts:\n",
    "                    # 제목에 수집 제외 문구 포함되었으면 무시.\n",
    "                    if pat_check(ban_list['main'], post['title']):\n",
    "                        continue\n",
    "                    post_id_list.append(post['post_id'])\n",
    "                posts_list.extend(post_id_list)\n",
    "            except Exception as exc:\n",
    "                logger.info('[ERROR] 게시글 목록 수집 중 에러 : ' + str(exc))\n",
    "                continue\n",
    "\n",
    "            logger.info('[SYSTEM] 게시글 상세 데이터 수집 시작.')\n",
    "            cnt = 0\n",
    "            for post in tqdm(posts):\n",
    "                if cnt > 50:\n",
    "                    make_excel(result_data, excel_name)\n",
    "                    result_data.clear()\n",
    "                    cnt = 0\n",
    "                try:\n",
    "                    cnt += 1\n",
    "                    post_temp = get_post_info(data['url'], club_id, post, blacklist=blacklist)\n",
    "                    if post_temp == False:\n",
    "                        continue\n",
    "                    result_data.append(post_temp)\n",
    "                except UnexpectedAlertPresentException:\n",
    "                    alert = driver.switch_to_alert()\n",
    "                    alert.accept()\n",
    "                    driver.implicitly_wait(3)\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                except Exception as exc:\n",
    "                    logger.info('[ERROR] 게시글 내용 수집 중 에러 : ' + str(exc))\n",
    "                    continue\n",
    "            make_excel(result_data, excel_name)\n",
    "            result_data.clear()\n",
    "        logger.info('[SYSTEM] 엑셀 파일 생성 완료.')\n",
    "        posts_list.extend(history)\n",
    "        _make_history(club_id, posts_list)\n",
    "        logger.info('[SYSTEM] 히스토리 파일 생성 완료.')\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
